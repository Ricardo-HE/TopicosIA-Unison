{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack - Q-Learning\n",
    "\n",
    "Este codigo esta basado en el ambiente que proporciona OpenAI en su libreria Gym en codigo [blackjack.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py).\n",
    "\n",
    "Y el algoritmo Q-Learning esta basado en el que esta descrito en el libro de Reinforcement learning: An introduction (second edition) de Richard S. Sutton y Andrew G. Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias\n",
    "\n",
    "* Plots se usará para graficar la recompensa a travez de los pasos\n",
    "* Distributions es para tener la distribución binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plots\n",
    "import Distributions\n",
    "Plots.pyplot();\n",
    "import Base: step, reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reglas\n",
    "A continuación se definiran las reglas para el juego y funciones para ejercerlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_natural (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "\n",
    "function cmp_(a, b)\n",
    "    return Float64(a > b) - Float64(a < b)\n",
    "end\n",
    "\n",
    "\n",
    "function draw_card()\n",
    "    return Int(rand(deck))\n",
    "end\n",
    "\n",
    "\n",
    "function draw_hand()\n",
    "    return [draw_card(), draw_card()]\n",
    "end\n",
    "\n",
    "\n",
    "function usable_ace(hand)  # Does this hand have a usable ace?\n",
    "    return 1 in hand && sum(hand) + 10 <= 21\n",
    "end\n",
    "\n",
    "\n",
    "function sum_hand(hand)  # Return current hand total\n",
    "    if usable_ace(hand)\n",
    "        return sum(hand) + 10\n",
    "    end\n",
    "    return sum(hand)\n",
    "end\n",
    "\n",
    "\n",
    "function is_bust(hand)  # Is this hand a bust?\n",
    "    return sum_hand(hand) > 21\n",
    "end\n",
    "    \n",
    "function score(hand)  # What is the score of this hand (0 if bust) \n",
    "    if is_bust(hand) return 22 else sum_hand(hand) end\n",
    "end\n",
    "\n",
    "\n",
    "function is_natural(hand)  # Is this hand a natural blackjack?\n",
    "    return sort(hand) == [1, 10]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estructura del ambiente del BlackJack\n",
    "\n",
    "Esta estructura es la que contiene el estado del juego y la función de paso (step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_start_terminal (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct BlackjackEnv\n",
    "    actions::Array{Int64,1}\n",
    "    natural::Bool\n",
    "    dealer::Array{Int64, 1}\n",
    "    player::Array{Int64, 1}\n",
    "    BlackjackEnv(n = false, player=draw_hand(), dealer=draw_hand()) = new(\n",
    "        [1,2],\n",
    "        n,\n",
    "        player,\n",
    "        dealer)\n",
    "end\n",
    "\n",
    "function _get_obs(self::BlackjackEnv)\n",
    "    return (sum_hand(self.player), sum_hand(self.dealer), usable_ace(self.player))\n",
    "end\n",
    "\n",
    "function reset(self::BlackjackEnv)\n",
    "    self.dealer = draw_hand()\n",
    "    self.player = draw_hand()\n",
    "    return _get_obs(self)\n",
    "end\n",
    "\n",
    "function step(self::BlackjackEnv, action::Int)\n",
    "    @assert action ∈ self.actions \"Action must be 2 (hit) or 1 (stick)\"\n",
    "    \n",
    "    # Because we don't want to modify the original self (we need the original self for the algorithm)\n",
    "    new_self = deepcopy(self)\n",
    "    if Bool(action-1)  # hit: add a card to players hand and return\n",
    "        append!(new_self.player, draw_card())\n",
    "        if is_bust(new_self.player)\n",
    "            done = true\n",
    "            reward = -1\n",
    "        else\n",
    "            done = false\n",
    "            reward = 0\n",
    "        end\n",
    "    else   # stick: play out the dealers hand, and score\n",
    "        done = true\n",
    "        while sum_hand(new_self.dealer) < 17\n",
    "            append!(new_self.dealer, draw_card())\n",
    "        end\n",
    "        reward = cmp_(score(new_self.player), score(new_self.dealer))\n",
    "        if new_self.natural && is_natural(new_self.player) && reward == 1\n",
    "            reward = 1.5\n",
    "        end\n",
    "    end\n",
    "    return (new_self, _get_obs(new_self), reward, done, Dict())\n",
    "end\n",
    "\n",
    "function is_start_terminal(self::BlackjackEnv)\n",
    "    p = score(self.player)\n",
    "    if p == 0 || p == 21\n",
    "        done = true\n",
    "    else\n",
    "        done = false\n",
    "    end\n",
    "    return done\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escoger acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_action (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability for exploration\n",
    "ε = 0.1\n",
    "\n",
    "# step size\n",
    "α = 0.5\n",
    "\n",
    "# gamma for Q-Learning and Expected Sarsa\n",
    "γ = 1\n",
    "\n",
    "# choose an action based on epsilon greedy algorithm\n",
    "function choose_action(state, q_value)\n",
    "    if rand(Distributions.Binomial(1, ε)) == 1\n",
    "        #println(\"Entro al random\")\n",
    "        action = rand(state.actions)\n",
    "    else\n",
    "        values_ = q_value[score(state.player), state.dealer[1], :]\n",
    "        action = rand([action_ for (action_, value_) in enumerate(values_) if value_ == maximum(values_)])\n",
    "    end\n",
    "    return action\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 2 methods)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an episode with Q-Learning\n",
    "# @q_value: values for state action pair, will be updated\n",
    "# @step_size: step size for updating\n",
    "# @return: total rewards within this episode\n",
    "function q_learning(q_value, step_size=α)\n",
    "    state = BlackjackEnv(true)\n",
    "    rewards = 0.0\n",
    "    done = is_start_terminal(state)\n",
    "    if done == true\n",
    "        if score(state.player) == 21\n",
    "            rewards == 500\n",
    "        end\n",
    "    end\n",
    "    while !done  #state != GOAL\n",
    "        action = choose_action(state, q_value)\n",
    "        next_state, obs, reward, done, _ = step(state, action)\n",
    "        rewards += reward\n",
    "        \n",
    "        #println(\"state: \", typeof(state))\n",
    "        #println(\"next_state: \", typeof(next_state))\n",
    "        #println(\"action: \", typeof(action))\n",
    "        # Q-Learning update\n",
    "        q_value[score(state.player), state.dealer[1], action] += step_size * (\n",
    "                reward + γ * maximum(q_value[score(next_state.player),next_state.dealer[1], :]) -\n",
    "                q_value[score(state.player), state.dealer[1], action])\n",
    "        state = next_state\n",
    "    end\n",
    "    return rewards\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "print_optimal_policy (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function print_optimal_policy(q_value)\n",
    "    optimal_policy = []\n",
    "    for i in range(2, stop=22)\n",
    "        push!(optimal_policy,[])\n",
    "        for j in range(1, stop=10)\n",
    "            if i >= 21\n",
    "                push!(optimal_policy[end], (i,j,\"End\"))\n",
    "                continue\n",
    "            end\n",
    "            bestAction = argmax(q_value[i, j, :])\n",
    "            if bestAction == 2\n",
    "                push!(optimal_policy[end], (i,j,\"Hit\"))\n",
    "            elseif bestAction == 1\n",
    "                push!(optimal_policy[end], (i,j,\"Stick\"))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    #println(optimal_policy)\n",
    "    for row in optimal_policy\n",
    "        println(row)\n",
    "    end\n",
    "    #return optimal_policy\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000-element Array{Float64,1}:\n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       " -1.0  \n",
       "  ⋮    \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.2  \n",
       " -0.175\n",
       " -0.175\n",
       " -0.15 "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use multiple runs instead of a single run and a sliding window\n",
    "# With a single run I failed to present a smooth curve\n",
    "# However the optimal policy converges well with a single run\n",
    "# Sarsa converges to the safe path, while Q-Learning converges to the optimal path\n",
    "\n",
    "# episodes of each run\n",
    "episodes = 1000000\n",
    "\n",
    "# perform 40 independent runs\n",
    "runs = 40\n",
    "\n",
    "rewards_q_learning = zeros(episodes)\n",
    "\n",
    "# 1 index - Score from the player\n",
    "# 2 index - First card from the dealer (because we don't know the other card)\n",
    "# 3 index - Number of actions\n",
    "q_q_learning = zeros(Float16, 22, 10, 2)\n",
    "\n",
    "for r in range(1, stop=runs)\n",
    "    q_q_learning = zeros(Float16, 22, 10, 2)\n",
    "    for i in range(1, stop=episodes)\n",
    "        # cut off the value by -100 to draw the figure more elegantly\n",
    "        # rewards_sarsa[i] += max(sarsa(q_sarsa), -100)\n",
    "        # rewards_q_learning[i] += max(q_learning(q_q_learning), -100)\n",
    "        rewards_q_learning[i] += q_learning(q_q_learning)\n",
    "    end\n",
    "end\n",
    "\n",
    "# averaging over independt runs\n",
    "rewards_q_learning /= runs\n",
    "sort(rewards_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning Optimal Policy:\n",
      "Any[(2, 1, \"Stick\"), (2, 2, \"Stick\"), (2, 3, \"Stick\"), (2, 4, \"Stick\"), (2, 5, \"Stick\"), (2, 6, \"Stick\"), (2, 7, \"Stick\"), (2, 8, \"Stick\"), (2, 9, \"Stick\"), (2, 10, \"Stick\")]\n",
      "Any[(3, 1, \"Stick\"), (3, 2, \"Stick\"), (3, 3, \"Stick\"), (3, 4, \"Stick\"), (3, 5, \"Stick\"), (3, 6, \"Stick\"), (3, 7, \"Stick\"), (3, 8, \"Stick\"), (3, 9, \"Stick\"), (3, 10, \"Stick\")]\n",
      "Any[(4, 1, \"Stick\"), (4, 2, \"Hit\"), (4, 3, \"Stick\"), (4, 4, \"Hit\"), (4, 5, \"Hit\"), (4, 6, \"Stick\"), (4, 7, \"Stick\"), (4, 8, \"Stick\"), (4, 9, \"Stick\"), (4, 10, \"Hit\")]\n",
      "Any[(5, 1, \"Stick\"), (5, 2, \"Hit\"), (5, 3, \"Stick\"), (5, 4, \"Stick\"), (5, 5, \"Stick\"), (5, 6, \"Hit\"), (5, 7, \"Stick\"), (5, 8, \"Stick\"), (5, 9, \"Hit\"), (5, 10, \"Hit\")]\n",
      "Any[(6, 1, \"Stick\"), (6, 2, \"Stick\"), (6, 3, \"Hit\"), (6, 4, \"Hit\"), (6, 5, \"Hit\"), (6, 6, \"Stick\"), (6, 7, \"Stick\"), (6, 8, \"Stick\"), (6, 9, \"Stick\"), (6, 10, \"Stick\")]\n",
      "Any[(7, 1, \"Stick\"), (7, 2, \"Stick\"), (7, 3, \"Stick\"), (7, 4, \"Stick\"), (7, 5, \"Stick\"), (7, 6, \"Hit\"), (7, 7, \"Stick\"), (7, 8, \"Stick\"), (7, 9, \"Hit\"), (7, 10, \"Stick\")]\n",
      "Any[(8, 1, \"Stick\"), (8, 2, \"Hit\"), (8, 3, \"Hit\"), (8, 4, \"Hit\"), (8, 5, \"Hit\"), (8, 6, \"Stick\"), (8, 7, \"Stick\"), (8, 8, \"Stick\"), (8, 9, \"Stick\"), (8, 10, \"Stick\")]\n",
      "Any[(9, 1, \"Stick\"), (9, 2, \"Stick\"), (9, 3, \"Hit\"), (9, 4, \"Hit\"), (9, 5, \"Hit\"), (9, 6, \"Stick\"), (9, 7, \"Stick\"), (9, 8, \"Stick\"), (9, 9, \"Stick\"), (9, 10, \"Stick\")]\n",
      "Any[(10, 1, \"Stick\"), (10, 2, \"Hit\"), (10, 3, \"Stick\"), (10, 4, \"Stick\"), (10, 5, \"Hit\"), (10, 6, \"Stick\"), (10, 7, \"Stick\"), (10, 8, \"Stick\"), (10, 9, \"Stick\"), (10, 10, \"Stick\")]\n",
      "Any[(11, 1, \"Stick\"), (11, 2, \"Hit\"), (11, 3, \"Stick\"), (11, 4, \"Hit\"), (11, 5, \"Hit\"), (11, 6, \"Stick\"), (11, 7, \"Stick\"), (11, 8, \"Stick\"), (11, 9, \"Stick\"), (11, 10, \"Stick\")]\n",
      "Any[(12, 1, \"Stick\"), (12, 2, \"Stick\"), (12, 3, \"Stick\"), (12, 4, \"Stick\"), (12, 5, \"Stick\"), (12, 6, \"Hit\"), (12, 7, \"Stick\"), (12, 8, \"Stick\"), (12, 9, \"Stick\"), (12, 10, \"Stick\")]\n",
      "Any[(13, 1, \"Stick\"), (13, 2, \"Stick\"), (13, 3, \"Stick\"), (13, 4, \"Stick\"), (13, 5, \"Hit\"), (13, 6, \"Hit\"), (13, 7, \"Stick\"), (13, 8, \"Stick\"), (13, 9, \"Stick\"), (13, 10, \"Stick\")]\n",
      "Any[(14, 1, \"Stick\"), (14, 2, \"Stick\"), (14, 3, \"Stick\"), (14, 4, \"Stick\"), (14, 5, \"Stick\"), (14, 6, \"Stick\"), (14, 7, \"Stick\"), (14, 8, \"Stick\"), (14, 9, \"Stick\"), (14, 10, \"Stick\")]\n",
      "Any[(15, 1, \"Hit\"), (15, 2, \"Stick\"), (15, 3, \"Stick\"), (15, 4, \"Hit\"), (15, 5, \"Stick\"), (15, 6, \"Hit\"), (15, 7, \"Stick\"), (15, 8, \"Stick\"), (15, 9, \"Stick\"), (15, 10, \"Stick\")]\n",
      "Any[(16, 1, \"Hit\"), (16, 2, \"Stick\"), (16, 3, \"Hit\"), (16, 4, \"Hit\"), (16, 5, \"Hit\"), (16, 6, \"Hit\"), (16, 7, \"Stick\"), (16, 8, \"Stick\"), (16, 9, \"Stick\"), (16, 10, \"Stick\")]\n",
      "Any[(17, 1, \"Stick\"), (17, 2, \"Stick\"), (17, 3, \"Stick\"), (17, 4, \"Hit\"), (17, 5, \"Hit\"), (17, 6, \"Stick\"), (17, 7, \"Stick\"), (17, 8, \"Stick\"), (17, 9, \"Stick\"), (17, 10, \"Stick\")]\n",
      "Any[(18, 1, \"Hit\"), (18, 2, \"Stick\"), (18, 3, \"Stick\"), (18, 4, \"Hit\"), (18, 5, \"Hit\"), (18, 6, \"Hit\"), (18, 7, \"Stick\"), (18, 8, \"Stick\"), (18, 9, \"Stick\"), (18, 10, \"Stick\")]\n",
      "Any[(19, 1, \"Stick\"), (19, 2, \"Hit\"), (19, 3, \"Stick\"), (19, 4, \"Stick\"), (19, 5, \"Stick\"), (19, 6, \"Hit\"), (19, 7, \"Stick\"), (19, 8, \"Stick\"), (19, 9, \"Stick\"), (19, 10, \"Stick\")]\n",
      "Any[(20, 1, \"Stick\"), (20, 2, \"Stick\"), (20, 3, \"Stick\"), (20, 4, \"Stick\"), (20, 5, \"Stick\"), (20, 6, \"Hit\"), (20, 7, \"Stick\"), (20, 8, \"Stick\"), (20, 9, \"Stick\"), (20, 10, \"Stick\")]\n",
      "Any[(21, 1, \"End\"), (21, 2, \"End\"), (21, 3, \"End\"), (21, 4, \"End\"), (21, 5, \"End\"), (21, 6, \"End\"), (21, 7, \"End\"), (21, 8, \"End\"), (21, 9, \"End\"), (21, 10, \"End\")]\n",
      "Any[(22, 1, \"End\"), (22, 2, \"End\"), (22, 3, \"End\"), (22, 4, \"End\"), (22, 5, \"End\"), (22, 6, \"End\"), (22, 7, \"End\"), (22, 8, \"End\"), (22, 9, \"End\"), (22, 10, \"End\")]\n"
     ]
    }
   ],
   "source": [
    "println(\"Q-Learning Optimal Policy:\")\n",
    "# (Puntaje del jugador, Carta visible del dealer, Accion del jugador)\n",
    "print_optimal_policy(q_q_learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
